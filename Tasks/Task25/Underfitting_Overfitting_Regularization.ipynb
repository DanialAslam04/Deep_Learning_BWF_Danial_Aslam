{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a890a5d3",
   "metadata": {},
   "source": [
    "# <-- Task 25 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f321056",
   "metadata": {},
   "source": [
    "## *Underfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00841e74",
   "metadata": {},
   "source": [
    "Underfitting occurs when a machine learning model fails to capture the underlying patterns and relationships in the data, resulting in poor performance. \n",
    "\n",
    "*Insufficient complexity*: The model is too simple to capture the complexities of the data.\n",
    "\n",
    "*High bias*: The model is biased towards making overly simplistic assumptions.\n",
    "\n",
    "###### *Solutions to address underfitting*:\n",
    "\n",
    "1. Increase model complexity: Use a more complex model with higher capacity, such as increasing the number of layers or neurons in a neural network.\n",
    "\n",
    "\n",
    "2. Feature engineering: Create more informative and relevant features to help the model better understand the data.\n",
    "\n",
    "\n",
    "3. Collect more data: A larger dataset can provide more diverse examples and help the model generalize better.\n",
    "\n",
    "\n",
    "4. Increase the number of training epochs\n",
    "\n",
    "\n",
    "5. Adjust hyperparameters: Tune the hyperparameters of the model, such as learning rate, regularization strength, or tree depth, to find a better balance between underfitting and overfitting.\n",
    "\n",
    "\n",
    "6. Use ensemble methods: Combine multiple models, such as bagging or boosting techniques, to reduce underfitting and improve performance.\n",
    "\n",
    "\n",
    "7. Reduce regularization: If the model is over-regularized, reducing the amount of regularization can help alleviate underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710eb3b6",
   "metadata": {},
   "source": [
    "## *Overfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafda063",
   "metadata": {},
   "source": [
    "Overfitting occurs when a machine learning model learns the training data too well, to the extent that it starts to memorize noise or irrelevant patterns. This results in poor performance on unseen data. \n",
    "\n",
    "\n",
    "High model complexity: The model is overly complex and can capture noise or random fluctuations in the training data.\n",
    "\n",
    "Low bias, high variance: The model has low bias, meaning it can fit the training data well, but high variance, leading to poor generalization to new data.\n",
    "\n",
    "\n",
    "###### * Solutions to address overfitting:*\n",
    "\n",
    "1. Increase training data: A larger and more diverse dataset can help the model generalize better and reduce the likelihood of overfitting.\n",
    "\n",
    "\n",
    "2. Model simplification: Simplify the model architecture, reducing its complexity by reducing the number of layers, neurons, or parameters.\n",
    "\n",
    "\n",
    "3. Regularization: Apply regularization techniques, such as L1 or L2 regularization, to penalize complex models and prevent overfitting.\n",
    "\n",
    "\n",
    "4. Feature selection: Choose a subset of relevant features that are most informative for the problem, eliminating irrelevant or redundant features.\n",
    "\n",
    "\n",
    "5. Cross-validation: Use techniques like k-fold cross-validation to assess the model's performance on multiple subsets of the data and identify potential overfitting.\n",
    "\n",
    "\n",
    "6. Early stopping: Monitor the model's performance on a validation set and stop training when the performance starts to deteriorate, thus preventing overfitting.\n",
    "\n",
    "\n",
    "7. Dropout: using dropout we can also reduce overfitting.\n",
    "\n",
    "\n",
    "8. Ensemble methods: Utilize ensemble techniques like bagging, random forests, or gradient boosting to combine multiple models and reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc15859",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e08eb",
   "metadata": {},
   "source": [
    "Regularization is a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more regular. This is called weight regularization, and it’s done by adding to the loss function of the network a cost associated with having large\n",
    "weights. This cost comes in two flavors:\n",
    "\n",
    "\n",
    " L1 regularization(Lasso)—The cost added is proportional to the absolute value of the\n",
    "weight coefficients (the L1 norm of the weights).\n",
    "\n",
    "\n",
    " L2 regularization(Ridge)—The cost added is proportional to the square of the value of the\n",
    "weight coefficients (the L2 norm of the weights). L2 regularization is also called\n",
    "weight decay in the context of neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9709174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import regularizers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da677b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
