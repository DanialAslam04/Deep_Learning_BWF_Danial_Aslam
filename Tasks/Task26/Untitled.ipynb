{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab0932d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd41337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0b4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5b63481",
   "metadata": {},
   "source": [
    "#  Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11d6ca",
   "metadata": {},
   "source": [
    "An activation function in deep learning is a mathematical function that introduces non-linearity to the output of a neural network layer. It determines the output of a neuron, allowing the neural network to learn complex patterns and make non-linear decisions. Here are some famous activation functions along with their main points and a small code snippet for each:\n",
    "\n",
    "### 1. Sigmoid Function:\n",
    "   - Outputs values between 0 and 1, representing probabilities.\n",
    "   - Smooth gradient but prone to vanishing gradient problem.\n",
    "   - Used in the output layer for binary classification problems.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def sigmoid(x):\n",
    "       return 1 / (1 + np.exp(-x))\n",
    "   ```\n",
    "\n",
    "### 2. ReLU (Rectified Linear Unit):\n",
    "   - Outputs the input directly if it is positive, otherwise outputs zero.\n",
    "   - Fast computation and avoids the vanishing gradient problem.\n",
    "   - Widely used in hidden layers of deep neural networks.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def relu(x):\n",
    "       return np.maximum(0, x)\n",
    "   ```\n",
    "\n",
    "### 3. Leaky ReLU:\n",
    "   - Similar to ReLU, but allows a small slope for negative values.\n",
    "   - Addresses the \"dying ReLU\" problem where neurons can become inactive.\n",
    "   - Can prevent gradient saturation and enable learning in more complex networks.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def leaky_relu(x, alpha=0.01):\n",
    "       return np.where(x > 0, x, x * alpha)\n",
    "   ```\n",
    "\n",
    "### 4. Tanh (Hyperbolic Tangent):\n",
    "   - Outputs values between -1 and 1, representing negative and positive values.\n",
    "   - Similar to the sigmoid function, but centered at zero.\n",
    "   - Used in recurrent neural networks (RNNs) and hidden layers.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def tanh(x):\n",
    "       return np.tanh(x)\n",
    "   ```\n",
    "\n",
    "### 5. Softmax:\n",
    "   - Used in the output layer for multi-class classification problems.\n",
    "   - Scales the outputs to represent class probabilities that sum up to 1.\n",
    "   - Suitable for mutually exclusive classes.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def softmax(x):\n",
    "       exps = np.exp(x - np.max(x))\n",
    "       return exps / np.sum(exps)\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea8a6f",
   "metadata": {},
   "source": [
    "## <................................<------------------------------------------------------->.................................>                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32cbfd",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15782622",
   "metadata": {},
   "source": [
    "A loss function in deep learning is a mathematical function that quantifies the discrepancy between the predicted output of a neural network and the true target values. It measures how well the network is performing on a given task and provides a signal for the network to adjust its parameters during the learning process. Here are some famous loss functions along with their main points and a small code snippet for each:\n",
    "\n",
    "### 1. Mean Squared Error (MSE):\n",
    "   - Calculates the average squared difference between predicted and true values.\n",
    "   - Sensitive to outliers due to the squared term.\n",
    "   - Commonly used for regression problems.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def mean_squared_error(y_true, y_pred):\n",
    "       return np.mean((y_true - y_pred) ** 2)\n",
    "   ```\n",
    "\n",
    "### 2. Binary Cross-Entropy:\n",
    "   - Measures the dissimilarity between predicted probabilities and true binary labels.\n",
    "   - Suitable for binary classification problems.\n",
    "   - Penalizes large errors more than the mean squared error loss.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def binary_cross_entropy(y_true, y_pred):\n",
    "       return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "   ```\n",
    "\n",
    "### 3. Categorical Cross-Entropy:\n",
    "   - Evaluates the dissimilarity between predicted class probabilities and true one-hot encoded labels.\n",
    "   - Applicable for multi-class classification problems.\n",
    "   - Encourages the network to assign high probabilities to the correct class.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def categorical_cross_entropy(y_true, y_pred):\n",
    "       return np.mean(-np.sum(y_true * np.log(y_pred), axis=1))\n",
    "   ```\n",
    "\n",
    "### 4. Kullback-Leibler Divergence (KL Divergence):\n",
    "   - Measures the difference between two probability distributions.\n",
    "   - Often used in variational autoencoders (VAEs) or generative models.\n",
    "   - Encourages the predicted distribution to match the true distribution.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def kl_divergence(p, q):\n",
    "       return np.sum(p * np.log(p / q))\n",
    "   ```\n",
    "\n",
    "### 5. Huber Loss:\n",
    "   - Combines the characteristics of squared error and absolute error.\n",
    "   - Less sensitive to outliers compared to MSE.\n",
    "   - Used for robust regression to balance between smoothness and robustness.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def huber_loss(y_true, y_pred, delta=1.0):\n",
    "       error = y_true - y_pred\n",
    "       quadratic_term = 0.5 * error**2\n",
    "       absolute_term = delta * (np.abs(error) - 0.5 * delta)\n",
    "       return np.mean(np.where(np.abs(error) <= delta, quadratic_term, absolute_term))\n",
    "   ```\n",
    "\n",
    "These are just a few examples of loss functions used in deep learning. Each loss function has its own characteristics and is suitable for different types of problems and network architectures. The choice of the loss function depends on the nature of the task and the desired behavior of the neural network during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297868d",
   "metadata": {},
   "source": [
    "## <................................<------------------------------------------------------->.................................>                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc70751",
   "metadata": {},
   "source": [
    "# Opimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12267bda",
   "metadata": {},
   "source": [
    "In deep learning, optimizers are algorithms used to adjust the parameters (weights and biases) of a neural network during the training process. They aim to minimize the loss function by iteratively updating the network's parameters based on the gradients of the loss with respect to those parameters. Here are some famous optimizers along with their main points and a small code snippet for each:\n",
    "\n",
    "### 1. Stochastic Gradient Descent (SGD):\n",
    "   - Updates the parameters using the gradients of the loss with respect to the parameters.\n",
    "   - Performs updates for each training example (or a mini-batch) individually.\n",
    "   - Simple and widely used optimizer.\n",
    "\n",
    "### 2. Adam (Adaptive Moment Estimation):\n",
    "   - Combines the advantages of AdaGrad and RMSProp algorithms.\n",
    "   - Adapts the learning rate for each parameter based on their first and second-order moments.\n",
    "   - Efficient and commonly used optimizer.\n",
    "\n",
    "\n",
    "### 3. RMSProp (Root Mean Square Propagation):\n",
    "   - Adapts the learning rate based on the recent magnitude of gradients for each parameter.\n",
    "   - Divides the learning rate by a running average of squared gradients.\n",
    "   - Well-suited for dealing with sparse data or noisy gradients.\n",
    "\n",
    "  \n",
    "### 4. Adagrad (Adaptive Gradient Algorithm):\n",
    "   - Adapts the learning rate based on the historical squared gradients for each parameter.\n",
    "   - Divides the learning rate by a running sum of squared gradients.\n",
    "   - Suitable for handling sparse data and frequent feature occurrences.\n",
    "\n",
    "### 5. Adamax:\n",
    "   - Variant of Adam optimizer that replaces the second moment estimation with the infinity norm.\n",
    "   - Performs well on models with sparse gradients or large parameter updates.\n",
    "   - Effective for deep learning models with recurrent structures.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeeacf5",
   "metadata": {},
   "source": [
    "# Evaluation Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf277a",
   "metadata": {},
   "source": [
    "Evaluation metrics in deep learning are used to assess the performance and effectiveness of a trained model on a specific task. These metrics provide quantitative measures of how well the model is performing, enabling comparison and selection of different models or tuning of hyperparameters. Here are some famous evaluation metrics along with their main points and a small code snippet for each:\n",
    "\n",
    "1. Accuracy:\n",
    "   - Measures the proportion of correctly classified samples.\n",
    "   - Commonly used for classification problems with balanced class distribution.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   def accuracy(y_true, y_pred):\n",
    "       correct = np.sum(y_true == y_pred)\n",
    "       total = len(y_true)\n",
    "       return correct / total\n",
    "   ```\n",
    "\n",
    "2. Precision, Recall, and F1-Score:\n",
    "   - Precision: Measures the proportion of true positive predictions out of all positive predictions.\n",
    "   - Recall: Measures the proportion of true positive predictions out of all actual positive samples.\n",
    "   - F1-Score: Harmonic mean of precision and recall, balances both metrics.\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "   precision = precision_score(y_true, y_pred)\n",
    "   recall = recall_score(y_true, y_pred)\n",
    "   f1 = f1_score(y_true, y_pred)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6fd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fbba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
